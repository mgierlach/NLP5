{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Natural Language Processing course PROJECT\n",
    "### Group S: Mateusz Gierlach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main goal of a project is creating a system that allows for sentiment analysis of tweets representing political opinions or opinions about some recent political events. System will be able to classify the tweet as positive or negative.\n",
    "\n",
    "Main problem I encountered was the lack of annotated (sentiment) database with political tweets. Therefore, I chose the approach of building and evaluating models on known datasets with the sentiment labelled. Then, I run the models on never-seen-before dataset of political tweets and conclude whether such system gives satisfactory results on data coming from different distributions.\n",
    "\n",
    "There are 3 separate parts of the project. In part 1, I use general IMDB reviews data to build word2vec model and classification (Random Forest) model. In part 2, I use Sentiment140 tweets database to build word2vec model and classification (Random Forest) model. In part 3, I peerform an inference of models from previous parts to see how well their performance is on political tweets. Then, I conclude my experiments and give conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: word2vec + RF classification on IMDB reviews data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build the general word2vec embeddings model on a widely popular IMDB reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from gensim.models import word2vec\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading in the IMDB datasets for training and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"imdb/labeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
    "test = pd.read_csv(\"imdb/testData.tsv\",header=0, delimiter=\"\\t\", quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading and loading tokenizers from NLTK library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/mg/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    }
   ],
   "source": [
    "import nltk.data\n",
    "nltk.download('popular')\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for converting (removing tags, deleting other characters, transformation to lower-case, optionally removing stopwords) a review to a list of separate words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_wordlist(review, remove_stopwords=False):\n",
    "    review_text = BeautifulSoup(review).get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\",\" \",review_text)\n",
    "    words = review_text.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))     \n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for converting a review to sentences, using the downloaded tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(review_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the above functions to parse all the reviews from training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mg/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:272: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/Users/mg/anaconda3/lib/python3.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initializing and building a word2vec model in gensim library. At the top, I specify parameters for the model. Then, I save the model for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-28 10:43:02,858 : INFO : collecting all words and their counts\n",
      "2019-04-28 10:43:02,860 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-28 10:43:02,915 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2019-04-28 10:43:02,994 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2019-04-28 10:43:03,066 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n",
      "2019-04-28 10:43:03,157 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2019-04-28 10:43:03,220 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2019-04-28 10:43:03,287 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2019-04-28 10:43:03,344 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2019-04-28 10:43:03,409 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2019-04-28 10:43:03,492 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2019-04-28 10:43:03,599 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2019-04-28 10:43:03,690 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2019-04-28 10:43:03,802 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2019-04-28 10:43:03,910 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2019-04-28 10:43:04,000 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2019-04-28 10:43:04,085 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2019-04-28 10:43:04,150 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2019-04-28 10:43:04,244 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2019-04-28 10:43:04,347 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2019-04-28 10:43:04,437 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2019-04-28 10:43:04,530 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2019-04-28 10:43:04,607 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2019-04-28 10:43:04,679 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2019-04-28 10:43:04,746 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2019-04-28 10:43:04,833 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2019-04-28 10:43:04,892 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2019-04-28 10:43:04,962 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2019-04-28 10:43:05,016 : INFO : collected 74218 word types from a corpus of 5920724 raw words and 266551 sentences\n",
      "2019-04-28 10:43:05,017 : INFO : Loading a fresh vocabulary\n",
      "2019-04-28 10:43:05,078 : INFO : min_count=40 retains 8306 unique words (11% of original 74218, drops 65912)\n",
      "2019-04-28 10:43:05,079 : INFO : min_count=40 leaves 5559777 word corpus (93% of original 5920724, drops 360947)\n",
      "2019-04-28 10:43:05,117 : INFO : deleting the raw counts dictionary of 74218 items\n",
      "2019-04-28 10:43:05,120 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2019-04-28 10:43:05,121 : INFO : downsampling leaves estimated 4044393 word corpus (72.7% of prior 5559777)\n",
      "2019-04-28 10:43:05,166 : INFO : estimated required memory for 8306 words and 300 dimensions: 24087400 bytes\n",
      "2019-04-28 10:43:05,167 : INFO : resetting layer weights\n",
      "2019-04-28 10:43:05,329 : INFO : training model with 4 workers on 8306 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-04-28 10:43:06,338 : INFO : EPOCH 1 - PROGRESS: at 11.36% examples, 462945 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:07,354 : INFO : EPOCH 1 - PROGRESS: at 23.20% examples, 466209 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:08,362 : INFO : EPOCH 1 - PROGRESS: at 39.71% examples, 532213 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:09,366 : INFO : EPOCH 1 - PROGRESS: at 56.64% examples, 568745 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:10,371 : INFO : EPOCH 1 - PROGRESS: at 73.13% examples, 588108 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-28 10:43:11,389 : INFO : EPOCH 1 - PROGRESS: at 84.84% examples, 567962 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:12,395 : INFO : EPOCH 1 - PROGRESS: at 96.70% examples, 554537 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:12,604 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-28 10:43:12,615 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-28 10:43:12,622 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-28 10:43:12,630 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-28 10:43:12,631 : INFO : EPOCH - 1 : training on 5920724 raw words (4044457 effective words) took 7.3s, 554506 effective words/s\n",
      "2019-04-28 10:43:13,662 : INFO : EPOCH 2 - PROGRESS: at 13.23% examples, 526656 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:14,666 : INFO : EPOCH 2 - PROGRESS: at 26.19% examples, 524497 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:15,668 : INFO : EPOCH 2 - PROGRESS: at 40.41% examples, 540494 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-28 10:43:16,682 : INFO : EPOCH 2 - PROGRESS: at 54.28% examples, 543066 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:17,682 : INFO : EPOCH 2 - PROGRESS: at 67.94% examples, 545084 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:18,713 : INFO : EPOCH 2 - PROGRESS: at 79.47% examples, 530001 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-28 10:43:19,716 : INFO : EPOCH 2 - PROGRESS: at 89.65% examples, 513621 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:20,355 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-28 10:43:20,356 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-28 10:43:20,368 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-28 10:43:20,371 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-28 10:43:20,372 : INFO : EPOCH - 2 : training on 5920724 raw words (4044556 effective words) took 7.7s, 522992 effective words/s\n",
      "2019-04-28 10:43:21,395 : INFO : EPOCH 3 - PROGRESS: at 13.85% examples, 556811 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-28 10:43:22,412 : INFO : EPOCH 3 - PROGRESS: at 25.53% examples, 509620 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-28 10:43:23,420 : INFO : EPOCH 3 - PROGRESS: at 35.83% examples, 477511 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-28 10:43:24,422 : INFO : EPOCH 3 - PROGRESS: at 46.28% examples, 463971 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-28 10:43:25,428 : INFO : EPOCH 3 - PROGRESS: at 62.37% examples, 499807 words/s, in_qsize 8, out_qsize 1\n",
      "2019-04-28 10:43:26,435 : INFO : EPOCH 3 - PROGRESS: at 78.81% examples, 527126 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:27,439 : INFO : EPOCH 3 - PROGRESS: at 95.40% examples, 546831 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:27,703 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-28 10:43:27,705 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-28 10:43:27,718 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-28 10:43:27,726 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-28 10:43:27,726 : INFO : EPOCH - 3 : training on 5920724 raw words (4044913 effective words) took 7.3s, 550566 effective words/s\n",
      "2019-04-28 10:43:28,739 : INFO : EPOCH 4 - PROGRESS: at 15.82% examples, 644080 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:29,740 : INFO : EPOCH 4 - PROGRESS: at 32.76% examples, 662607 words/s, in_qsize 7, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-28 10:43:30,755 : INFO : EPOCH 4 - PROGRESS: at 49.74% examples, 666198 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:31,764 : INFO : EPOCH 4 - PROGRESS: at 63.04% examples, 633014 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:32,771 : INFO : EPOCH 4 - PROGRESS: at 79.47% examples, 639183 words/s, in_qsize 7, out_qsize 1\n",
      "2019-04-28 10:43:33,780 : INFO : EPOCH 4 - PROGRESS: at 93.81% examples, 628288 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:34,146 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-28 10:43:34,148 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-28 10:43:34,160 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-28 10:43:34,167 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-28 10:43:34,167 : INFO : EPOCH - 4 : training on 5920724 raw words (4045400 effective words) took 6.4s, 628747 effective words/s\n",
      "2019-04-28 10:43:35,181 : INFO : EPOCH 5 - PROGRESS: at 15.34% examples, 622352 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:36,192 : INFO : EPOCH 5 - PROGRESS: at 31.24% examples, 627906 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:37,195 : INFO : EPOCH 5 - PROGRESS: at 46.79% examples, 627363 words/s, in_qsize 8, out_qsize 0\n",
      "2019-04-28 10:43:38,207 : INFO : EPOCH 5 - PROGRESS: at 63.20% examples, 634133 words/s, in_qsize 7, out_qsize 1\n",
      "2019-04-28 10:43:39,221 : INFO : EPOCH 5 - PROGRESS: at 79.30% examples, 636565 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:40,224 : INFO : EPOCH 5 - PROGRESS: at 95.40% examples, 637984 words/s, in_qsize 7, out_qsize 0\n",
      "2019-04-28 10:43:40,492 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-04-28 10:43:40,497 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-28 10:43:40,507 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-28 10:43:40,515 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-28 10:43:40,516 : INFO : EPOCH - 5 : training on 5920724 raw words (4044897 effective words) took 6.3s, 637870 effective words/s\n",
      "2019-04-28 10:43:40,516 : INFO : training on a 29603620 raw words (20224223 effective words) took 35.2s, 574782 effective words/s\n",
      "2019-04-28 10:43:40,518 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-04-28 10:43:40,619 : INFO : saving Word2Vec object under imdb_model, separately None\n",
      "2019-04-28 10:43:40,620 : INFO : not storing attribute vectors_norm\n",
      "2019-04-28 10:43:40,621 : INFO : not storing attribute cum_table\n",
      "2019-04-28 10:43:40,623 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-28 10:43:40,867 : INFO : saved imdb_model\n"
     ]
    }
   ],
   "source": [
    "num_features = 300\n",
    "min_word_count = 40\n",
    "num_workers = 4\n",
    "context = 10\n",
    "downsampling = 1e-3\n",
    "\n",
    "model1 = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "model1.init_sims(replace=True)\n",
    "model1_name = \"imdb_model\"\n",
    "model1.save(model1_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's move to building a classification model for sentiment.\n",
    "\n",
    "First, we need methods for getting an averaged feature vector for each review. Sentiment can be defined for the whole review/tweet/document, as an average of words from whole text.\n",
    "\n",
    "Below, there are methods for averaging these reviews, based on the words used, represented through word2vec model. Stopwords usually should be removed only at this stage, as semantics (word2vec) can be learned with it. But we don't use them at the stage of averaging reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureVecMethod(words, model, num_features):\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1   \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the above methods to get average features for my training dataset. This is the preparation step before fitting the classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mg/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clean_train_reviews = []\n",
    "for review in train['review']:\n",
    "    clean_train_reviews.append(review_wordlist(review, remove_stopwords=True)) \n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model1, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same thing for test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mg/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_wordlist(review,remove_stopwords=True))\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model1, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the Random Forest classification model for sentiment prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest1 = RandomForestClassifier(n_estimators = 100)  \n",
    "forest1 = forest1.fit(trainDataVecs, train[\"sentiment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the built RF model to predict sentiment classes on a test set. Let's take a look how the first 15 test reviews have been classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\"Naturally in a film who's main themes are of ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"This movie is a disaster within a disaster fi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"All in all, this is a movie for kids. We saw ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Afraid of the Dark left me with the impressio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\"A very accurate depiction of small time mob l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\"...as valuable as King Tut's tomb! (OK, maybe...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\"This has to be one of the biggest misfires ev...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\"This is one of those movies I watched, and wo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\"The worst movie i've seen in years (and i've ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\"Five medical students (Kevin Bacon, David Lab...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\"'The Mill on the Floss' was one of the lesser...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>\"I just saw this film at the phoenix film fest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"\\\"The Love Letter\\\" is one of those movies th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>\"Another fantastic offering from the Monkey Is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>\"This was included on the disk \\\"Shorts: Volum...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               review  sentiment\n",
       "0   \"Naturally in a film who's main themes are of ...          1\n",
       "1   \"This movie is a disaster within a disaster fi...          0\n",
       "2   \"All in all, this is a movie for kids. We saw ...          1\n",
       "3   \"Afraid of the Dark left me with the impressio...          0\n",
       "4   \"A very accurate depiction of small time mob l...          1\n",
       "5   \"...as valuable as King Tut's tomb! (OK, maybe...          1\n",
       "6   \"This has to be one of the biggest misfires ev...          0\n",
       "7   \"This is one of those movies I watched, and wo...          0\n",
       "8   \"The worst movie i've seen in years (and i've ...          0\n",
       "9   \"Five medical students (Kevin Bacon, David Lab...          1\n",
       "10  \"'The Mill on the Floss' was one of the lesser...          1\n",
       "11  \"I just saw this film at the phoenix film fest...          1\n",
       "12  \"\\\"The Love Letter\\\" is one of those movies th...          1\n",
       "13  \"Another fantastic offering from the Monkey Is...          1\n",
       "14  \"This was included on the disk \\\"Shorts: Volum...          0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1 = forest1.predict(testDataVecs)\n",
    "output1 = pd.DataFrame(data = {\"review\": test[\"review\"], \"sentiment\": result1})\n",
    "output1.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the predicted outputs to a CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "output1.to_csv(\"output_imdb.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I do not validate the model at this stage. I use all the annotated data for training. Validation is performed in part 3 for political tweets, as this is the main point of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: word2vec + RF classification on Sentiment140 Twitter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, let's build a word2vec model and classification model on a popular Sentiment140 sentiment-annotated dataset.\n",
    "\n",
    "Importing needed libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "from copy import deepcopy\n",
    "from string import punctuation\n",
    "from random import shuffle\n",
    "import pickle\n",
    "import h5py\n",
    "import json\n",
    "import matplotlib.pyplot as plt \n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas(desc=\"progress-bar\")\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk import word_tokenize\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading training dataset from Sentiment140:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest():\n",
    "    data = pd.read_csv('sentiment140/tweetstrain.csv', encoding='latin-1')\n",
    "    data.columns=[\"Sentiment\",\"ItemID\",\"Date\",\"Blank\",\"SentimentSource\",\"SentimentText\"]\n",
    "    data.drop(['ItemID', 'SentimentSource'], axis=1, inplace=True)\n",
    "    data = data[data.Sentiment.isnull() == False]\n",
    "    data['Sentiment'] = data['Sentiment'].map( {4:1, 0:0}) #Converting 4 to 1\n",
    "    data = data[data['SentimentText'].isnull() == False]\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', axis=1, inplace=True)\n",
    "    return data\n",
    "\n",
    "data = ingest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data before building the model. Below, functions for tokenization and filtering the tweets and for additional processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "def tokenize(tweet):\n",
    "    try:\n",
    "        tweet = tweet.lower()\n",
    "        tokens = tokenizer.tokenize(tweet)\n",
    "        tokens = list(filter(lambda t: not t.startswith('@'), tokens))\n",
    "        tokens = list(filter(lambda t: not t.startswith('#'), tokens))\n",
    "        tokens = list(filter(lambda t: not t.startswith('http'), tokens))\n",
    "        return tokens\n",
    "    except:\n",
    "        return 'NC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def postprocess(data):\n",
    "    data['tokens'] = data['SentimentText'].progress_map(tokenize)\n",
    "    data = data[data.tokens != 'NC']\n",
    "    data.reset_index(inplace=True)\n",
    "    data.drop('index', inplace=True, axis=1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "progress-bar: 100%|██████████| 1599999/1599999 [01:39<00:00, 16098.14it/s]\n"
     ]
    }
   ],
   "source": [
    "data = postprocess(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the data into training and testing sets, before building a word2vec model. I use the first million examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(np.array(data.head(1000000).tokens),\n",
    "                                                np.array(data.head(1000000).Sentiment), test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for labelling the data into training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabeledSentence = gensim.models.doc2vec.LabeledSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelizeTweets(tweets, label_type):\n",
    "    labelized = []\n",
    "    for i,v in tqdm(enumerate(tweets)):\n",
    "        label = '%s_%s'%(label_type,i)\n",
    "        labelized.append(LabeledSentence(v, [label]))\n",
    "    return labelized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the above methods to label the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/mg/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \"\"\"\n",
      "800000it [00:07, 103476.50it/s]\n",
      "200000it [00:00, 234105.68it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train = labelizeTweets(x_train, 'TRAIN')\n",
    "x_test = labelizeTweets(x_test, 'TEST') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/mg/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `LabeledSentence` (Class will be removed in 4.0.0, use TaggedDocument instead).\n",
      "  \"\"\"\n",
      "1599999it [00:15, 104106.12it/s]\n"
     ]
    }
   ],
   "source": [
    "data_labellised= labelizeTweets(np.array(data.tokens), 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build the word2vec model on labellized tweets. As previously, I use gensim package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1599999/1599999 [00:00<00:00, 2331007.57it/s]\n",
      "2019-04-28 11:57:32,107 : INFO : collecting all words and their counts\n",
      "2019-04-28 11:57:32,109 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-28 11:57:32,170 : INFO : PROGRESS: at sentence #10000, processed 151409 words, keeping 13541 word types\n",
      "2019-04-28 11:57:32,236 : INFO : PROGRESS: at sentence #20000, processed 300464 words, keeping 20437 word types\n",
      "2019-04-28 11:57:32,301 : INFO : PROGRESS: at sentence #30000, processed 448553 words, keeping 26065 word types\n",
      "2019-04-28 11:57:32,363 : INFO : PROGRESS: at sentence #40000, processed 598296 words, keeping 30941 word types\n",
      "2019-04-28 11:57:32,437 : INFO : PROGRESS: at sentence #50000, processed 749113 words, keeping 35737 word types\n",
      "2019-04-28 11:57:32,508 : INFO : PROGRESS: at sentence #60000, processed 899949 words, keeping 39931 word types\n",
      "2019-04-28 11:57:32,586 : INFO : PROGRESS: at sentence #70000, processed 1048619 words, keeping 43592 word types\n",
      "2019-04-28 11:57:32,667 : INFO : PROGRESS: at sentence #80000, processed 1198313 words, keeping 47210 word types\n",
      "2019-04-28 11:57:32,736 : INFO : PROGRESS: at sentence #90000, processed 1352421 words, keeping 50808 word types\n",
      "2019-04-28 11:57:32,834 : INFO : PROGRESS: at sentence #100000, processed 1504945 words, keeping 54115 word types\n",
      "2019-04-28 11:57:32,905 : INFO : PROGRESS: at sentence #110000, processed 1656058 words, keeping 57331 word types\n",
      "2019-04-28 11:57:32,973 : INFO : PROGRESS: at sentence #120000, processed 1805990 words, keeping 60596 word types\n",
      "2019-04-28 11:57:33,045 : INFO : PROGRESS: at sentence #130000, processed 1956805 words, keeping 63727 word types\n",
      "2019-04-28 11:57:33,113 : INFO : PROGRESS: at sentence #140000, processed 2110335 words, keeping 66698 word types\n",
      "2019-04-28 11:57:33,189 : INFO : PROGRESS: at sentence #150000, processed 2265872 words, keeping 69778 word types\n",
      "2019-04-28 11:57:33,281 : INFO : PROGRESS: at sentence #160000, processed 2418697 words, keeping 72559 word types\n",
      "2019-04-28 11:57:33,372 : INFO : PROGRESS: at sentence #170000, processed 2571571 words, keeping 75352 word types\n",
      "2019-04-28 11:57:33,448 : INFO : PROGRESS: at sentence #180000, processed 2722257 words, keeping 77950 word types\n",
      "2019-04-28 11:57:33,516 : INFO : PROGRESS: at sentence #190000, processed 2872047 words, keeping 80419 word types\n",
      "2019-04-28 11:57:33,613 : INFO : PROGRESS: at sentence #200000, processed 3025232 words, keeping 83137 word types\n",
      "2019-04-28 11:57:33,682 : INFO : PROGRESS: at sentence #210000, processed 3176796 words, keeping 85722 word types\n",
      "2019-04-28 11:57:33,755 : INFO : PROGRESS: at sentence #220000, processed 3326460 words, keeping 88191 word types\n",
      "2019-04-28 11:57:33,813 : INFO : PROGRESS: at sentence #230000, processed 3473879 words, keeping 90434 word types\n",
      "2019-04-28 11:57:33,907 : INFO : PROGRESS: at sentence #240000, processed 3626207 words, keeping 93015 word types\n",
      "2019-04-28 11:57:33,972 : INFO : PROGRESS: at sentence #250000, processed 3776244 words, keeping 95375 word types\n",
      "2019-04-28 11:57:34,037 : INFO : PROGRESS: at sentence #260000, processed 3925443 words, keeping 97423 word types\n",
      "2019-04-28 11:57:34,101 : INFO : PROGRESS: at sentence #270000, processed 4076214 words, keeping 99666 word types\n",
      "2019-04-28 11:57:34,165 : INFO : PROGRESS: at sentence #280000, processed 4228934 words, keeping 102114 word types\n",
      "2019-04-28 11:57:34,227 : INFO : PROGRESS: at sentence #290000, processed 4379890 words, keeping 104200 word types\n",
      "2019-04-28 11:57:34,293 : INFO : PROGRESS: at sentence #300000, processed 4530478 words, keeping 106259 word types\n",
      "2019-04-28 11:57:34,360 : INFO : PROGRESS: at sentence #310000, processed 4682589 words, keeping 108279 word types\n",
      "2019-04-28 11:57:34,426 : INFO : PROGRESS: at sentence #320000, processed 4834908 words, keeping 110551 word types\n",
      "2019-04-28 11:57:34,495 : INFO : PROGRESS: at sentence #330000, processed 4987103 words, keeping 112710 word types\n",
      "2019-04-28 11:57:34,561 : INFO : PROGRESS: at sentence #340000, processed 5139574 words, keeping 114917 word types\n",
      "2019-04-28 11:57:34,629 : INFO : PROGRESS: at sentence #350000, processed 5293264 words, keeping 117109 word types\n",
      "2019-04-28 11:57:34,707 : INFO : PROGRESS: at sentence #360000, processed 5445722 words, keeping 119106 word types\n",
      "2019-04-28 11:57:34,812 : INFO : PROGRESS: at sentence #370000, processed 5596028 words, keeping 121043 word types\n",
      "2019-04-28 11:57:34,887 : INFO : PROGRESS: at sentence #380000, processed 5745764 words, keeping 122997 word types\n",
      "2019-04-28 11:57:34,965 : INFO : PROGRESS: at sentence #390000, processed 5898557 words, keeping 125249 word types\n",
      "2019-04-28 11:57:35,032 : INFO : PROGRESS: at sentence #400000, processed 6049224 words, keeping 127257 word types\n",
      "2019-04-28 11:57:35,098 : INFO : PROGRESS: at sentence #410000, processed 6198948 words, keeping 129144 word types\n",
      "2019-04-28 11:57:35,162 : INFO : PROGRESS: at sentence #420000, processed 6348260 words, keeping 131053 word types\n",
      "2019-04-28 11:57:35,234 : INFO : PROGRESS: at sentence #430000, processed 6498598 words, keeping 133169 word types\n",
      "2019-04-28 11:57:35,299 : INFO : PROGRESS: at sentence #440000, processed 6648037 words, keeping 135113 word types\n",
      "2019-04-28 11:57:35,365 : INFO : PROGRESS: at sentence #450000, processed 6797065 words, keeping 136925 word types\n",
      "2019-04-28 11:57:35,427 : INFO : PROGRESS: at sentence #460000, processed 6945596 words, keeping 138716 word types\n",
      "2019-04-28 11:57:35,491 : INFO : PROGRESS: at sentence #470000, processed 7097022 words, keeping 140689 word types\n",
      "2019-04-28 11:57:35,586 : INFO : PROGRESS: at sentence #480000, processed 7247818 words, keeping 142656 word types\n",
      "2019-04-28 11:57:35,655 : INFO : PROGRESS: at sentence #490000, processed 7397303 words, keeping 144436 word types\n",
      "2019-04-28 11:57:35,729 : INFO : PROGRESS: at sentence #500000, processed 7546085 words, keeping 146212 word types\n",
      "2019-04-28 11:57:35,800 : INFO : PROGRESS: at sentence #510000, processed 7697978 words, keeping 147945 word types\n",
      "2019-04-28 11:57:35,873 : INFO : PROGRESS: at sentence #520000, processed 7850379 words, keeping 150053 word types\n",
      "2019-04-28 11:57:35,960 : INFO : PROGRESS: at sentence #530000, processed 8001861 words, keeping 151839 word types\n",
      "2019-04-28 11:57:36,031 : INFO : PROGRESS: at sentence #540000, processed 8152467 words, keeping 153528 word types\n",
      "2019-04-28 11:57:36,114 : INFO : PROGRESS: at sentence #550000, processed 8304658 words, keeping 155258 word types\n",
      "2019-04-28 11:57:36,236 : INFO : PROGRESS: at sentence #560000, processed 8456401 words, keeping 157131 word types\n",
      "2019-04-28 11:57:36,321 : INFO : PROGRESS: at sentence #570000, processed 8609588 words, keeping 158826 word types\n",
      "2019-04-28 11:57:36,392 : INFO : PROGRESS: at sentence #580000, processed 8761908 words, keeping 160523 word types\n",
      "2019-04-28 11:57:36,456 : INFO : PROGRESS: at sentence #590000, processed 8914473 words, keeping 162140 word types\n",
      "2019-04-28 11:57:36,520 : INFO : PROGRESS: at sentence #600000, processed 9067653 words, keeping 163929 word types\n",
      "2019-04-28 11:57:36,582 : INFO : PROGRESS: at sentence #610000, processed 9219682 words, keeping 165688 word types\n",
      "2019-04-28 11:57:36,650 : INFO : PROGRESS: at sentence #620000, processed 9372548 words, keeping 167335 word types\n",
      "2019-04-28 11:57:36,713 : INFO : PROGRESS: at sentence #630000, processed 9523806 words, keeping 168962 word types\n",
      "2019-04-28 11:57:36,775 : INFO : PROGRESS: at sentence #640000, processed 9676226 words, keeping 170678 word types\n",
      "2019-04-28 11:57:36,836 : INFO : PROGRESS: at sentence #650000, processed 9830526 words, keeping 172649 word types\n",
      "2019-04-28 11:57:36,899 : INFO : PROGRESS: at sentence #660000, processed 9984107 words, keeping 174373 word types\n",
      "2019-04-28 11:57:36,971 : INFO : PROGRESS: at sentence #670000, processed 10137156 words, keeping 176018 word types\n",
      "2019-04-28 11:57:37,043 : INFO : PROGRESS: at sentence #680000, processed 10287530 words, keeping 177727 word types\n",
      "2019-04-28 11:57:37,109 : INFO : PROGRESS: at sentence #690000, processed 10438564 words, keeping 179499 word types\n",
      "2019-04-28 11:57:37,174 : INFO : PROGRESS: at sentence #700000, processed 10589452 words, keeping 181248 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-28 11:57:37,235 : INFO : PROGRESS: at sentence #710000, processed 10739146 words, keeping 182771 word types\n",
      "2019-04-28 11:57:37,295 : INFO : PROGRESS: at sentence #720000, processed 10888974 words, keeping 184357 word types\n",
      "2019-04-28 11:57:37,354 : INFO : PROGRESS: at sentence #730000, processed 11039031 words, keeping 185936 word types\n",
      "2019-04-28 11:57:37,415 : INFO : PROGRESS: at sentence #740000, processed 11190160 words, keeping 187684 word types\n",
      "2019-04-28 11:57:37,474 : INFO : PROGRESS: at sentence #750000, processed 11341059 words, keeping 189366 word types\n",
      "2019-04-28 11:57:37,534 : INFO : PROGRESS: at sentence #760000, processed 11492647 words, keeping 190808 word types\n",
      "2019-04-28 11:57:37,590 : INFO : PROGRESS: at sentence #770000, processed 11643905 words, keeping 192396 word types\n",
      "2019-04-28 11:57:37,638 : INFO : PROGRESS: at sentence #780000, processed 11796213 words, keeping 194001 word types\n",
      "2019-04-28 11:57:37,687 : INFO : PROGRESS: at sentence #790000, processed 11949661 words, keeping 195823 word types\n",
      "2019-04-28 11:57:37,735 : INFO : PROGRESS: at sentence #800000, processed 12102097 words, keeping 197455 word types\n",
      "2019-04-28 11:57:37,786 : INFO : PROGRESS: at sentence #810000, processed 12246413 words, keeping 199487 word types\n",
      "2019-04-28 11:57:37,830 : INFO : PROGRESS: at sentence #820000, processed 12387702 words, keeping 201373 word types\n",
      "2019-04-28 11:57:37,872 : INFO : PROGRESS: at sentence #830000, processed 12528473 words, keeping 203251 word types\n",
      "2019-04-28 11:57:37,930 : INFO : PROGRESS: at sentence #840000, processed 12669282 words, keeping 205233 word types\n",
      "2019-04-28 11:57:37,982 : INFO : PROGRESS: at sentence #850000, processed 12812830 words, keeping 207240 word types\n",
      "2019-04-28 11:57:38,031 : INFO : PROGRESS: at sentence #860000, processed 12957773 words, keeping 209207 word types\n",
      "2019-04-28 11:57:38,080 : INFO : PROGRESS: at sentence #870000, processed 13102206 words, keeping 211182 word types\n",
      "2019-04-28 11:57:38,125 : INFO : PROGRESS: at sentence #880000, processed 13245627 words, keeping 213002 word types\n",
      "2019-04-28 11:57:38,172 : INFO : PROGRESS: at sentence #890000, processed 13387593 words, keeping 214906 word types\n",
      "2019-04-28 11:57:38,214 : INFO : PROGRESS: at sentence #900000, processed 13531267 words, keeping 216621 word types\n",
      "2019-04-28 11:57:38,260 : INFO : PROGRESS: at sentence #910000, processed 13673446 words, keeping 218539 word types\n",
      "2019-04-28 11:57:38,315 : INFO : PROGRESS: at sentence #920000, processed 13814858 words, keeping 220221 word types\n",
      "2019-04-28 11:57:38,361 : INFO : PROGRESS: at sentence #930000, processed 13956260 words, keeping 221901 word types\n",
      "2019-04-28 11:57:38,409 : INFO : PROGRESS: at sentence #940000, processed 14100733 words, keeping 223787 word types\n",
      "2019-04-28 11:57:38,459 : INFO : PROGRESS: at sentence #950000, processed 14243423 words, keeping 225495 word types\n",
      "2019-04-28 11:57:38,506 : INFO : PROGRESS: at sentence #960000, processed 14386946 words, keeping 227499 word types\n",
      "2019-04-28 11:57:38,551 : INFO : PROGRESS: at sentence #970000, processed 14526668 words, keeping 229176 word types\n",
      "2019-04-28 11:57:38,607 : INFO : PROGRESS: at sentence #980000, processed 14670478 words, keeping 230981 word types\n",
      "2019-04-28 11:57:38,657 : INFO : PROGRESS: at sentence #990000, processed 14814822 words, keeping 233053 word types\n",
      "2019-04-28 11:57:38,715 : INFO : PROGRESS: at sentence #1000000, processed 14961764 words, keeping 234747 word types\n",
      "2019-04-28 11:57:38,778 : INFO : PROGRESS: at sentence #1010000, processed 15104776 words, keeping 236713 word types\n",
      "2019-04-28 11:57:38,837 : INFO : PROGRESS: at sentence #1020000, processed 15245145 words, keeping 238523 word types\n",
      "2019-04-28 11:57:38,896 : INFO : PROGRESS: at sentence #1030000, processed 15388099 words, keeping 240106 word types\n",
      "2019-04-28 11:57:38,947 : INFO : PROGRESS: at sentence #1040000, processed 15532102 words, keeping 241793 word types\n",
      "2019-04-28 11:57:39,027 : INFO : PROGRESS: at sentence #1050000, processed 15674276 words, keeping 243573 word types\n",
      "2019-04-28 11:57:39,125 : INFO : PROGRESS: at sentence #1060000, processed 15815789 words, keeping 245097 word types\n",
      "2019-04-28 11:57:39,179 : INFO : PROGRESS: at sentence #1070000, processed 15956042 words, keeping 246664 word types\n",
      "2019-04-28 11:57:39,236 : INFO : PROGRESS: at sentence #1080000, processed 16095839 words, keeping 248271 word types\n",
      "2019-04-28 11:57:39,295 : INFO : PROGRESS: at sentence #1090000, processed 16238206 words, keeping 249990 word types\n",
      "2019-04-28 11:57:39,372 : INFO : PROGRESS: at sentence #1100000, processed 16381229 words, keeping 251910 word types\n",
      "2019-04-28 11:57:39,469 : INFO : PROGRESS: at sentence #1110000, processed 16523764 words, keeping 253703 word types\n",
      "2019-04-28 11:57:39,580 : INFO : PROGRESS: at sentence #1120000, processed 16662922 words, keeping 255308 word types\n",
      "2019-04-28 11:57:39,641 : INFO : PROGRESS: at sentence #1130000, processed 16802085 words, keeping 256971 word types\n",
      "2019-04-28 11:57:39,698 : INFO : PROGRESS: at sentence #1140000, processed 16944157 words, keeping 258539 word types\n",
      "2019-04-28 11:57:39,748 : INFO : PROGRESS: at sentence #1150000, processed 17085254 words, keeping 260132 word types\n",
      "2019-04-28 11:57:39,796 : INFO : PROGRESS: at sentence #1160000, processed 17229929 words, keeping 261825 word types\n",
      "2019-04-28 11:57:39,845 : INFO : PROGRESS: at sentence #1170000, processed 17373087 words, keeping 263622 word types\n",
      "2019-04-28 11:57:39,900 : INFO : PROGRESS: at sentence #1180000, processed 17515376 words, keeping 265268 word types\n",
      "2019-04-28 11:57:39,945 : INFO : PROGRESS: at sentence #1190000, processed 17657230 words, keeping 266753 word types\n",
      "2019-04-28 11:57:39,990 : INFO : PROGRESS: at sentence #1200000, processed 17799079 words, keeping 268284 word types\n",
      "2019-04-28 11:57:40,040 : INFO : PROGRESS: at sentence #1210000, processed 17939899 words, keeping 269790 word types\n",
      "2019-04-28 11:57:40,091 : INFO : PROGRESS: at sentence #1220000, processed 18086027 words, keeping 271587 word types\n",
      "2019-04-28 11:57:40,140 : INFO : PROGRESS: at sentence #1230000, processed 18232258 words, keeping 273284 word types\n",
      "2019-04-28 11:57:40,184 : INFO : PROGRESS: at sentence #1240000, processed 18378475 words, keeping 274832 word types\n",
      "2019-04-28 11:57:40,226 : INFO : PROGRESS: at sentence #1250000, processed 18522336 words, keeping 276358 word types\n",
      "2019-04-28 11:57:40,272 : INFO : PROGRESS: at sentence #1260000, processed 18664218 words, keeping 277808 word types\n",
      "2019-04-28 11:57:40,317 : INFO : PROGRESS: at sentence #1270000, processed 18808827 words, keeping 279268 word types\n",
      "2019-04-28 11:57:40,365 : INFO : PROGRESS: at sentence #1280000, processed 18954086 words, keeping 280836 word types\n",
      "2019-04-28 11:57:40,414 : INFO : PROGRESS: at sentence #1290000, processed 19097877 words, keeping 282598 word types\n",
      "2019-04-28 11:57:40,462 : INFO : PROGRESS: at sentence #1300000, processed 19244526 words, keeping 284216 word types\n",
      "2019-04-28 11:57:40,512 : INFO : PROGRESS: at sentence #1310000, processed 19388656 words, keeping 285658 word types\n",
      "2019-04-28 11:57:40,556 : INFO : PROGRESS: at sentence #1320000, processed 19534766 words, keeping 287356 word types\n",
      "2019-04-28 11:57:40,603 : INFO : PROGRESS: at sentence #1330000, processed 19679576 words, keeping 289060 word types\n",
      "2019-04-28 11:57:40,653 : INFO : PROGRESS: at sentence #1340000, processed 19823879 words, keeping 290691 word types\n",
      "2019-04-28 11:57:40,702 : INFO : PROGRESS: at sentence #1350000, processed 19968155 words, keeping 292344 word types\n",
      "2019-04-28 11:57:40,753 : INFO : PROGRESS: at sentence #1360000, processed 20108999 words, keeping 293831 word types\n",
      "2019-04-28 11:57:40,807 : INFO : PROGRESS: at sentence #1370000, processed 20249284 words, keeping 295302 word types\n",
      "2019-04-28 11:57:40,856 : INFO : PROGRESS: at sentence #1380000, processed 20392624 words, keeping 296690 word types\n",
      "2019-04-28 11:57:40,911 : INFO : PROGRESS: at sentence #1390000, processed 20538329 words, keeping 298346 word types\n",
      "2019-04-28 11:57:40,961 : INFO : PROGRESS: at sentence #1400000, processed 20681596 words, keeping 299985 word types\n",
      "2019-04-28 11:57:41,004 : INFO : PROGRESS: at sentence #1410000, processed 20822953 words, keeping 301514 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-28 11:57:41,060 : INFO : PROGRESS: at sentence #1420000, processed 20964663 words, keeping 302907 word types\n",
      "2019-04-28 11:57:41,110 : INFO : PROGRESS: at sentence #1430000, processed 21103510 words, keeping 304359 word types\n",
      "2019-04-28 11:57:41,157 : INFO : PROGRESS: at sentence #1440000, processed 21244727 words, keeping 305666 word types\n",
      "2019-04-28 11:57:41,197 : INFO : PROGRESS: at sentence #1450000, processed 21389765 words, keeping 307103 word types\n",
      "2019-04-28 11:57:41,247 : INFO : PROGRESS: at sentence #1460000, processed 21533732 words, keeping 308767 word types\n",
      "2019-04-28 11:57:41,300 : INFO : PROGRESS: at sentence #1470000, processed 21676377 words, keeping 310340 word types\n",
      "2019-04-28 11:57:41,349 : INFO : PROGRESS: at sentence #1480000, processed 21819577 words, keeping 311736 word types\n",
      "2019-04-28 11:57:41,394 : INFO : PROGRESS: at sentence #1490000, processed 21959556 words, keeping 313123 word types\n",
      "2019-04-28 11:57:41,443 : INFO : PROGRESS: at sentence #1500000, processed 22100635 words, keeping 314470 word types\n",
      "2019-04-28 11:57:41,491 : INFO : PROGRESS: at sentence #1510000, processed 22242826 words, keeping 315852 word types\n",
      "2019-04-28 11:57:41,540 : INFO : PROGRESS: at sentence #1520000, processed 22386039 words, keeping 317550 word types\n",
      "2019-04-28 11:57:41,589 : INFO : PROGRESS: at sentence #1530000, processed 22528277 words, keeping 319172 word types\n",
      "2019-04-28 11:57:41,637 : INFO : PROGRESS: at sentence #1540000, processed 22671483 words, keeping 320703 word types\n",
      "2019-04-28 11:57:41,685 : INFO : PROGRESS: at sentence #1550000, processed 22814148 words, keeping 322039 word types\n",
      "2019-04-28 11:57:41,734 : INFO : PROGRESS: at sentence #1560000, processed 22956766 words, keeping 323351 word types\n",
      "2019-04-28 11:57:41,782 : INFO : PROGRESS: at sentence #1570000, processed 23099720 words, keeping 324748 word types\n",
      "2019-04-28 11:57:41,834 : INFO : PROGRESS: at sentence #1580000, processed 23242697 words, keeping 326308 word types\n",
      "2019-04-28 11:57:41,881 : INFO : PROGRESS: at sentence #1590000, processed 23385623 words, keeping 327952 word types\n",
      "2019-04-28 11:57:41,938 : INFO : collected 329496 word types from a corpus of 23529566 raw words and 1599999 sentences\n",
      "2019-04-28 11:57:41,941 : INFO : Loading a fresh vocabulary\n",
      "2019-04-28 11:57:42,199 : INFO : min_count=10 retains 35390 unique words (10% of original 329496, drops 294106)\n",
      "2019-04-28 11:57:42,200 : INFO : min_count=10 leaves 23021437 word corpus (97% of original 23529566, drops 508129)\n",
      "2019-04-28 11:57:42,320 : INFO : deleting the raw counts dictionary of 329496 items\n",
      "2019-04-28 11:57:42,329 : INFO : sample=0.001 downsamples 54 most-common words\n",
      "2019-04-28 11:57:42,330 : INFO : downsampling leaves estimated 17108458 word corpus (74.3% of prior 23021437)\n",
      "2019-04-28 11:57:42,449 : INFO : estimated required memory for 35390 words and 200 dimensions: 74319000 bytes\n",
      "2019-04-28 11:57:42,450 : INFO : resetting layer weights\n",
      "100%|██████████| 1599999/1599999 [00:00<00:00, 2606359.15it/s]\n",
      "/Users/mg/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  \n",
      "2019-04-28 11:57:43,654 : INFO : training model with 3 workers on 35390 vocabulary and 200 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-28 11:57:44,670 : INFO : EPOCH 1 - PROGRESS: at 3.37% examples, 588866 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:45,679 : INFO : EPOCH 1 - PROGRESS: at 7.21% examples, 630251 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:46,682 : INFO : EPOCH 1 - PROGRESS: at 11.31% examples, 662097 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:47,684 : INFO : EPOCH 1 - PROGRESS: at 16.00% examples, 701838 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:48,694 : INFO : EPOCH 1 - PROGRESS: at 20.66% examples, 724885 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:49,715 : INFO : EPOCH 1 - PROGRESS: at 25.18% examples, 735014 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:50,716 : INFO : EPOCH 1 - PROGRESS: at 28.45% examples, 711206 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:51,717 : INFO : EPOCH 1 - PROGRESS: at 32.39% examples, 708805 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:52,722 : INFO : EPOCH 1 - PROGRESS: at 36.49% examples, 710537 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:53,726 : INFO : EPOCH 1 - PROGRESS: at 40.06% examples, 702537 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:54,727 : INFO : EPOCH 1 - PROGRESS: at 43.92% examples, 700862 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:55,737 : INFO : EPOCH 1 - PROGRESS: at 47.98% examples, 701299 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:56,748 : INFO : EPOCH 1 - PROGRESS: at 51.87% examples, 698069 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:57,762 : INFO : EPOCH 1 - PROGRESS: at 55.36% examples, 688909 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:58,778 : INFO : EPOCH 1 - PROGRESS: at 59.78% examples, 690975 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:57:59,780 : INFO : EPOCH 1 - PROGRESS: at 64.15% examples, 692885 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:00,780 : INFO : EPOCH 1 - PROGRESS: at 67.85% examples, 687743 words/s, in_qsize 4, out_qsize 1\n",
      "2019-04-28 11:58:01,786 : INFO : EPOCH 1 - PROGRESS: at 71.87% examples, 685827 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:02,786 : INFO : EPOCH 1 - PROGRESS: at 75.77% examples, 683726 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:58:03,790 : INFO : EPOCH 1 - PROGRESS: at 79.57% examples, 681325 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:04,799 : INFO : EPOCH 1 - PROGRESS: at 83.52% examples, 680536 words/s, in_qsize 4, out_qsize 1\n",
      "2019-04-28 11:58:05,800 : INFO : EPOCH 1 - PROGRESS: at 87.85% examples, 682178 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:58:06,804 : INFO : EPOCH 1 - PROGRESS: at 91.59% examples, 679284 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:07,831 : INFO : EPOCH 1 - PROGRESS: at 95.37% examples, 676288 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:08,836 : INFO : EPOCH 1 - PROGRESS: at 99.08% examples, 673760 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:09,150 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-28 11:58:09,153 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-28 11:58:09,159 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-28 11:58:09,160 : INFO : EPOCH - 1 : training on 23529566 raw words (17108645 effective words) took 25.5s, 671224 effective words/s\n",
      "2019-04-28 11:58:10,171 : INFO : EPOCH 2 - PROGRESS: at 2.96% examples, 514483 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:11,184 : INFO : EPOCH 2 - PROGRESS: at 6.97% examples, 606186 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:12,195 : INFO : EPOCH 2 - PROGRESS: at 11.02% examples, 641872 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:13,202 : INFO : EPOCH 2 - PROGRESS: at 15.29% examples, 667472 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:14,211 : INFO : EPOCH 2 - PROGRESS: at 19.59% examples, 684402 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:15,213 : INFO : EPOCH 2 - PROGRESS: at 23.45% examples, 684397 words/s, in_qsize 4, out_qsize 1\n",
      "2019-04-28 11:58:16,215 : INFO : EPOCH 2 - PROGRESS: at 26.90% examples, 672800 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:17,232 : INFO : EPOCH 2 - PROGRESS: at 30.86% examples, 673913 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-28 11:58:18,234 : INFO : EPOCH 2 - PROGRESS: at 34.28% examples, 666030 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:19,249 : INFO : EPOCH 2 - PROGRESS: at 37.81% examples, 661113 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:20,256 : INFO : EPOCH 2 - PROGRESS: at 40.46% examples, 643755 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:21,261 : INFO : EPOCH 2 - PROGRESS: at 43.01% examples, 627560 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:22,268 : INFO : EPOCH 2 - PROGRESS: at 46.21% examples, 622043 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:58:23,311 : INFO : EPOCH 2 - PROGRESS: at 49.33% examples, 615224 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:24,315 : INFO : EPOCH 2 - PROGRESS: at 53.23% examples, 617465 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-28 11:58:25,317 : INFO : EPOCH 2 - PROGRESS: at 56.94% examples, 617298 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:26,320 : INFO : EPOCH 2 - PROGRESS: at 61.19% examples, 622160 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:27,323 : INFO : EPOCH 2 - PROGRESS: at 65.63% examples, 628361 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-28 11:58:28,339 : INFO : EPOCH 2 - PROGRESS: at 69.87% examples, 631099 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:58:29,340 : INFO : EPOCH 2 - PROGRESS: at 74.19% examples, 634936 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:30,346 : INFO : EPOCH 2 - PROGRESS: at 78.61% examples, 639759 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:58:31,348 : INFO : EPOCH 2 - PROGRESS: at 81.28% examples, 631185 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:32,355 : INFO : EPOCH 2 - PROGRESS: at 86.03% examples, 637866 words/s, in_qsize 4, out_qsize 1\n",
      "2019-04-28 11:58:33,371 : INFO : EPOCH 2 - PROGRESS: at 89.58% examples, 635415 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:34,375 : INFO : EPOCH 2 - PROGRESS: at 93.92% examples, 638680 words/s, in_qsize 4, out_qsize 1\n",
      "2019-04-28 11:58:35,382 : INFO : EPOCH 2 - PROGRESS: at 98.47% examples, 642897 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:36,069 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-28 11:58:36,115 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-28 11:58:36,124 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-28 11:58:36,125 : INFO : EPOCH - 2 : training on 23529566 raw words (17107488 effective words) took 27.0s, 634639 effective words/s\n",
      "2019-04-28 11:58:37,162 : INFO : EPOCH 3 - PROGRESS: at 2.00% examples, 343420 words/s, in_qsize 5, out_qsize 1\n",
      "2019-04-28 11:58:38,171 : INFO : EPOCH 3 - PROGRESS: at 4.96% examples, 427504 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:39,185 : INFO : EPOCH 3 - PROGRESS: at 8.17% examples, 472042 words/s, in_qsize 4, out_qsize 1\n",
      "2019-04-28 11:58:40,202 : INFO : EPOCH 3 - PROGRESS: at 11.48% examples, 498875 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:41,204 : INFO : EPOCH 3 - PROGRESS: at 14.60% examples, 507998 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:42,215 : INFO : EPOCH 3 - PROGRESS: at 17.73% examples, 514815 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:43,239 : INFO : EPOCH 3 - PROGRESS: at 19.76% examples, 490880 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:44,248 : INFO : EPOCH 3 - PROGRESS: at 22.66% examples, 493722 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:45,256 : INFO : EPOCH 3 - PROGRESS: at 26.23% examples, 507734 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:46,266 : INFO : EPOCH 3 - PROGRESS: at 29.82% examples, 519045 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:58:47,275 : INFO : EPOCH 3 - PROGRESS: at 32.06% examples, 507330 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:48,283 : INFO : EPOCH 3 - PROGRESS: at 36.29% examples, 526913 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:58:49,285 : INFO : EPOCH 3 - PROGRESS: at 40.30% examples, 540978 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:50,292 : INFO : EPOCH 3 - PROGRESS: at 44.63% examples, 556483 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:51,305 : INFO : EPOCH 3 - PROGRESS: at 48.76% examples, 567225 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:52,310 : INFO : EPOCH 3 - PROGRESS: at 53.06% examples, 576770 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:53,310 : INFO : EPOCH 3 - PROGRESS: at 57.64% examples, 587469 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:54,316 : INFO : EPOCH 3 - PROGRESS: at 62.09% examples, 595607 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:55,317 : INFO : EPOCH 3 - PROGRESS: at 66.83% examples, 605131 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:56,325 : INFO : EPOCH 3 - PROGRESS: at 71.43% examples, 612081 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:57,333 : INFO : EPOCH 3 - PROGRESS: at 76.02% examples, 618880 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:58:58,339 : INFO : EPOCH 3 - PROGRESS: at 80.64% examples, 625844 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:58:59,352 : INFO : EPOCH 3 - PROGRESS: at 85.50% examples, 633553 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:00,362 : INFO : EPOCH 3 - PROGRESS: at 90.15% examples, 638897 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:01,370 : INFO : EPOCH 3 - PROGRESS: at 94.58% examples, 642496 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:59:02,380 : INFO : EPOCH 3 - PROGRESS: at 99.30% examples, 647627 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:02,519 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-28 11:59:02,520 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-28 11:59:02,534 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-28 11:59:02,535 : INFO : EPOCH - 3 : training on 23529566 raw words (17107795 effective words) took 26.4s, 648236 effective words/s\n",
      "2019-04-28 11:59:03,601 : INFO : EPOCH 4 - PROGRESS: at 4.38% examples, 764518 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:04,611 : INFO : EPOCH 4 - PROGRESS: at 9.06% examples, 793968 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:05,613 : INFO : EPOCH 4 - PROGRESS: at 13.67% examples, 800272 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:06,617 : INFO : EPOCH 4 - PROGRESS: at 18.32% examples, 803803 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:07,625 : INFO : EPOCH 4 - PROGRESS: at 22.71% examples, 797795 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:08,626 : INFO : EPOCH 4 - PROGRESS: at 27.27% examples, 798149 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:59:09,634 : INFO : EPOCH 4 - PROGRESS: at 31.93% examples, 799727 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:10,636 : INFO : EPOCH 4 - PROGRESS: at 36.37% examples, 797963 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:59:11,639 : INFO : EPOCH 4 - PROGRESS: at 40.75% examples, 795702 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:59:12,641 : INFO : EPOCH 4 - PROGRESS: at 45.17% examples, 793719 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:13,643 : INFO : EPOCH 4 - PROGRESS: at 49.74% examples, 794819 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:14,651 : INFO : EPOCH 4 - PROGRESS: at 52.54% examples, 766773 words/s, in_qsize 6, out_qsize 0\n",
      "2019-04-28 11:59:15,658 : INFO : EPOCH 4 - PROGRESS: at 56.23% examples, 754756 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:16,661 : INFO : EPOCH 4 - PROGRESS: at 60.13% examples, 746730 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:17,661 : INFO : EPOCH 4 - PROGRESS: at 64.45% examples, 744702 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:18,669 : INFO : EPOCH 4 - PROGRESS: at 69.38% examples, 748182 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:19,669 : INFO : EPOCH 4 - PROGRESS: at 73.09% examples, 739887 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:20,670 : INFO : EPOCH 4 - PROGRESS: at 76.50% examples, 730245 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:21,683 : INFO : EPOCH 4 - PROGRESS: at 80.43% examples, 726118 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:22,683 : INFO : EPOCH 4 - PROGRESS: at 83.95% examples, 719669 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:23,707 : INFO : EPOCH 4 - PROGRESS: at 87.45% examples, 712264 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:24,725 : INFO : EPOCH 4 - PROGRESS: at 90.75% examples, 704076 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:25,732 : INFO : EPOCH 4 - PROGRESS: at 95.19% examples, 705161 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:26,706 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-28 11:59:26,708 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-28 11:59:26,719 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-28 11:59:26,720 : INFO : EPOCH - 4 : training on 23529566 raw words (17108885 effective words) took 24.1s, 709379 effective words/s\n",
      "2019-04-28 11:59:27,740 : INFO : EPOCH 5 - PROGRESS: at 4.54% examples, 783840 words/s, in_qsize 6, out_qsize 1\n",
      "2019-04-28 11:59:28,740 : INFO : EPOCH 5 - PROGRESS: at 9.26% examples, 810536 words/s, in_qsize 5, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-28 11:59:29,745 : INFO : EPOCH 5 - PROGRESS: at 13.29% examples, 776547 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:30,747 : INFO : EPOCH 5 - PROGRESS: at 16.88% examples, 739088 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:31,748 : INFO : EPOCH 5 - PROGRESS: at 19.67% examples, 690672 words/s, in_qsize 5, out_qsize 1\n",
      "2019-04-28 11:59:32,783 : INFO : EPOCH 5 - PROGRESS: at 23.29% examples, 678696 words/s, in_qsize 4, out_qsize 1\n",
      "2019-04-28 11:59:33,825 : INFO : EPOCH 5 - PROGRESS: at 26.52% examples, 659011 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:34,830 : INFO : EPOCH 5 - PROGRESS: at 30.32% examples, 659146 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:35,845 : INFO : EPOCH 5 - PROGRESS: at 35.11% examples, 678368 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:36,846 : INFO : EPOCH 5 - PROGRESS: at 39.07% examples, 681073 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:37,860 : INFO : EPOCH 5 - PROGRESS: at 42.22% examples, 669313 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:38,868 : INFO : EPOCH 5 - PROGRESS: at 45.75% examples, 664569 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:39,879 : INFO : EPOCH 5 - PROGRESS: at 50.33% examples, 674958 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:40,885 : INFO : EPOCH 5 - PROGRESS: at 54.23% examples, 672485 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:41,889 : INFO : EPOCH 5 - PROGRESS: at 59.08% examples, 680940 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:42,900 : INFO : EPOCH 5 - PROGRESS: at 63.75% examples, 686306 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:43,901 : INFO : EPOCH 5 - PROGRESS: at 67.89% examples, 685691 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:44,916 : INFO : EPOCH 5 - PROGRESS: at 71.65% examples, 681182 words/s, in_qsize 4, out_qsize 1\n",
      "2019-04-28 11:59:45,926 : INFO : EPOCH 5 - PROGRESS: at 75.94% examples, 682358 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:46,934 : INFO : EPOCH 5 - PROGRESS: at 79.57% examples, 678457 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:47,942 : INFO : EPOCH 5 - PROGRESS: at 83.91% examples, 680917 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:48,944 : INFO : EPOCH 5 - PROGRESS: at 87.98% examples, 680576 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:49,953 : INFO : EPOCH 5 - PROGRESS: at 91.93% examples, 679196 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:50,956 : INFO : EPOCH 5 - PROGRESS: at 96.67% examples, 683415 words/s, in_qsize 5, out_qsize 0\n",
      "2019-04-28 11:59:51,711 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-28 11:59:51,722 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-28 11:59:51,726 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-28 11:59:51,726 : INFO : EPOCH - 5 : training on 23529566 raw words (17109487 effective words) took 25.0s, 684426 effective words/s\n",
      "2019-04-28 11:59:51,727 : INFO : training on a 117647830 raw words (85542300 effective words) took 128.1s, 667948 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(85542300, 117647830)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n=1000000\n",
    "n_dim = 200\n",
    "tweet_w2v = Word2Vec(size=n_dim, min_count=10)\n",
    "tweet_w2v.build_vocab([x.words for x in tqdm(data_labellised)])\n",
    "tweet_w2v.train([x.words for x in tqdm(data_labellised)], total_examples = tweet_w2v.corpus_count, \n",
    "                epochs = tweet_w2v.iter) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, save and load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-28 11:59:51,784 : INFO : saving Word2Vec object under w2vmodel, separately None\n",
      "2019-04-28 11:59:51,789 : INFO : not storing attribute vectors_norm\n",
      "2019-04-28 11:59:51,790 : INFO : not storing attribute cum_table\n",
      "2019-04-28 11:59:51,794 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-28 11:59:52,716 : INFO : saved w2vmodel\n"
     ]
    }
   ],
   "source": [
    "tweet_w2v.save('w2vmodel')\n",
    "#new_w2vmodel = gensim.models.Word2Vec.load('w2vmodel')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, I will weight the word vectors by their tf-idf values, so that I can also take into account the words that are specific to each document/tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(analyzer=lambda x: x, min_df=10)\n",
    "matrix = vectorizer.fit_transform([x.words for x in data_labellised])\n",
    "tfidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for building the feature vectors, on which we will build the classification model. I use both word2vec values and tf-idf values for determination of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildWordVector(tokens, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += tweet_w2v[word].reshape((1, size)) * tfidf[word]\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the above method to build the feature vectors for all tweets in training and testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/Users/mg/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "800000it [03:07, 4277.77it/s]\n",
      "0it [00:00, ?it/s]/Users/mg/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "200000it [00:48, 4146.09it/s]\n"
     ]
    }
   ],
   "source": [
    "train_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_train))])\n",
    "train_vecs_w2v = scale(train_vecs_w2v)\n",
    "\n",
    "test_vecs_w2v = np.concatenate([buildWordVector(z, n_dim) for z in tqdm(map(lambda x: x.words, x_test))])\n",
    "test_vecs_w2v = scale(test_vecs_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the Random Forest classifier on the built feature vectors. Here, I only load the model built previously, uncomment the code if you want to build the model here from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forest2 = RandomForestClassifier(n_estimators = 100)   \n",
    "#forest2 = forest2.fit(train_vecs_w2v, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for saving the RF model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filename = 'finalized_model.sav'\n",
    "#pickle.dump(forest2, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for loading the RF model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'finalized_model.sav'\n",
    "forest2 = pickle.load(open(filename, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what's the accuracy of the RF model on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.79369\n"
     ]
    }
   ],
   "source": [
    "score2 = forest2.score(X = test_vecs_w2v, y = y_test)\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before analyzing real tweets, let's quickly test the model on some possible, easy-to-classify query tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_sentiment(query):\n",
    "    n_dim = 200\n",
    "    querytokens=tokenize(query)\n",
    "    query_vecs_w2v = buildWordVector(querytokens, n_dim)\n",
    "    pred = forest2.predict(query_vecs_w2v).item()\n",
    "    sent = \"\"\n",
    "    if(pred == 0):\n",
    "        sent = \"NEGATIVE\"\n",
    "    elif(pred == 1):\n",
    "        sent = \"POSITIVE\"\n",
    "    print(query, \" - \", sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate Trump's policies.  -  NEGATIVE\n",
      "Incredible legislation  -  POSITIVE\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mg/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/Users/mg/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "query1 = \"I hate Trump's policies.\"\n",
    "query2 = \"Incredible legislation\"\n",
    "\n",
    "query_sentiment(query1)\n",
    "query_sentiment(query2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Inference on politics-related tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's test both RF models (forest1 trained on IMDB reviews, forest2 trained on tweets) on political tweets. I draw the test set of 30 tweets from bigger politics-related database and validate their sentiment myself, by-hand. Then, I take a look how predictions of both models compare to my own predictions and therefore draw conclusions about whether the models are useful for the task of predicting sentiment in political tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the dataset of political tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "kt = pd.read_csv(\"keyword-tweets.csv\", header=None)\n",
    "kt.columns = [\"type\", \"text\"]\n",
    "kt = kt[kt.type == \"POLIT\"]\n",
    "kt.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drawing tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "political_tweets = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPA Jackson: people want to keep out cold and save on energy bills. \"Environmentalism seems like enclave of the well-off.\" #AGIF\n"
     ]
    }
   ],
   "source": [
    "for j in range(30):\n",
    "    i = random.randint(0, kt.shape[0]-1)\n",
    "    query = kt[\"text\"][i]\n",
    "    print(query)\n",
    "    political_tweets.append(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I create the final DataFrame where I store text of 30 tweets and predictions from both models and my personal evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_df = pd.DataFrame(np.array(political_tweets), columns=[\"tweet\"])\n",
    "pt_df[\"imdb_model_pred\"] = -1\n",
    "pt_df[\"s140_model_pred\"] = -1\n",
    "pt_df[\"my_own_eval\"] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I run both RF prediction models on final 30 political tweets and also make my own evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# my evaluation\n",
    "my_evaluation = [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0]\n",
    "pt_df[\"my_own_eval\"] = my_evaluation\n",
    "\n",
    "#forest1 - IMDB-trained RF model\n",
    "clean_test_reviews = []\n",
    "for review in pt_df[\"tweet\"]:\n",
    "    clean_test_reviews.append(review_wordlist(review,remove_stopwords=True))\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model1, num_features)\n",
    "pt_df[\"imdb_model_pred\"] = forest1.predict(testDataVecs)\n",
    "\n",
    "#forest2 - Sentiment140-trained RF model\n",
    "for i in range(pt_df.shape[0]):\n",
    "    tw = pt_df[\"tweet\"][i]\n",
    "    query = tw\n",
    "    n_dim = 200\n",
    "    querytokens=tokenize(query)\n",
    "    query_vecs_w2v = buildWordVector(querytokens, n_dim)\n",
    "    pred = forest2.predict(query_vecs_w2v).item()\n",
    "    pt_df[\"s140_model_pred\"][i] = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>imdb_model_pred</th>\n",
       "      <th>s140_model_pred</th>\n",
       "      <th>my_own_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Mark_Meed: Breaking: Obama Official Linked...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Democrat stimulus plan is a mechanism whos...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is still trying to figure out why Obama got th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @Lyn_Sue: RT @TellTheTruth1 http://bit.ly/v...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@lauraflyme We need to destroy every company t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>YOU crazies are what give women and \"feminism\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>RT @worldprayr: U.S. soldier captured in Afgha...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RT @billmaher: I'd give a weeks pay 4 10min w/...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>@Dumb_Ox new congress report out, cf my post t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Strange activity,Twitter says account suspende...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RT Ed Chen yet another Obama left wing radical...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>RT @stoptheliberalsplease join the fight. we n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>RT @wyclef: RT @Fun_All_Day: President Clinton...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>@senjohnmccain Are you for or against a govern...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>First reading: Obama's Pacific ambitions play ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Thank you!! #tcot #tlot #cleansweep #patriot R...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>@Ubumbo Lou dobbs says avatar is a liberal plo...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>RT @OTOOLEFAN Government #hcr is so BAD that w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>RT @GStephanopoulos: Graham: Obama 'Timid and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The Public Isn't Buying Obama's Smooth Talk.. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>@Kasons4 I'm sure Obama takes at least 5 backu...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>RT @JeffAtwater \"I, however, place economy amo...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Who can tax the sunshine sprinkle it with goo ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Who the hell shined the spotlight on Sarah Pal...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>says B.S. to that. Conservatives only want to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Pass it on Peeps! RT @GovMikeHuckabee Congress...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>@LOA_Lover to do everything I possibly can 2 t...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>@VetsOnTheWatch Libs must hate it when conserv...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>#inaug09 Soros: Obama Improves Openness: A fin...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>EPA Jackson: people want to keep out cold and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  imdb_model_pred  \\\n",
       "0   RT @Mark_Meed: Breaking: Obama Official Linked...                1   \n",
       "1   The Democrat stimulus plan is a mechanism whos...                1   \n",
       "2   is still trying to figure out why Obama got th...                0   \n",
       "3   RT @Lyn_Sue: RT @TellTheTruth1 http://bit.ly/v...                0   \n",
       "4   @lauraflyme We need to destroy every company t...                0   \n",
       "5   YOU crazies are what give women and \"feminism\"...                0   \n",
       "6   RT @worldprayr: U.S. soldier captured in Afgha...                0   \n",
       "7   RT @billmaher: I'd give a weeks pay 4 10min w/...                1   \n",
       "8   @Dumb_Ox new congress report out, cf my post t...                1   \n",
       "9   Strange activity,Twitter says account suspende...                1   \n",
       "10  RT Ed Chen yet another Obama left wing radical...                1   \n",
       "11  RT @stoptheliberalsplease join the fight. we n...                0   \n",
       "12  RT @wyclef: RT @Fun_All_Day: President Clinton...                1   \n",
       "13  @senjohnmccain Are you for or against a govern...                1   \n",
       "14  First reading: Obama's Pacific ambitions play ...                0   \n",
       "15  Thank you!! #tcot #tlot #cleansweep #patriot R...                1   \n",
       "16  @Ubumbo Lou dobbs says avatar is a liberal plo...                1   \n",
       "17  RT @OTOOLEFAN Government #hcr is so BAD that w...                0   \n",
       "18  RT @GStephanopoulos: Graham: Obama 'Timid and ...                1   \n",
       "19  The Public Isn't Buying Obama's Smooth Talk.. ...                0   \n",
       "20  @Kasons4 I'm sure Obama takes at least 5 backu...                0   \n",
       "21  RT @JeffAtwater \"I, however, place economy amo...                0   \n",
       "22  Who can tax the sunshine sprinkle it with goo ...                1   \n",
       "23  Who the hell shined the spotlight on Sarah Pal...                0   \n",
       "24  says B.S. to that. Conservatives only want to ...                0   \n",
       "25  Pass it on Peeps! RT @GovMikeHuckabee Congress...                0   \n",
       "26  @LOA_Lover to do everything I possibly can 2 t...                1   \n",
       "27  @VetsOnTheWatch Libs must hate it when conserv...                1   \n",
       "28  #inaug09 Soros: Obama Improves Openness: A fin...                1   \n",
       "29  EPA Jackson: people want to keep out cold and ...                1   \n",
       "\n",
       "    s140_model_pred  my_own_eval  \n",
       "0                 0            0  \n",
       "1                 0            0  \n",
       "2                 0            0  \n",
       "3                 0            0  \n",
       "4                 0            0  \n",
       "5                 0            0  \n",
       "6                 0            1  \n",
       "7                 0            0  \n",
       "8                 0            1  \n",
       "9                 0            0  \n",
       "10                0            0  \n",
       "11                0            0  \n",
       "12                0            1  \n",
       "13                0            0  \n",
       "14                0            1  \n",
       "15                0            1  \n",
       "16                0            0  \n",
       "17                0            0  \n",
       "18                0            0  \n",
       "19                0            0  \n",
       "20                0            0  \n",
       "21                1            1  \n",
       "22                0            0  \n",
       "23                0            0  \n",
       "24                0            0  \n",
       "25                0            1  \n",
       "26                0            1  \n",
       "27                0            0  \n",
       "28                0            1  \n",
       "29                0            0  "
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of model accuracy, when compared with my own \"predictions\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "for1 = 0\n",
    "for2 = 0\n",
    "for k in range(pt_df.shape[0]):\n",
    "    if(pt_df[\"imdb_model_pred\"][k] == pt_df[\"my_own_eval\"][k]):\n",
    "        for1 += 1\n",
    "    if(pt_df[\"s140_model_pred\"][k] == pt_df[\"my_own_eval\"][k]):\n",
    "        for2 += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the accuracy of models for sentiment prediction of political tweets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'for1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b294c934bb4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Estimated accuracy of IMDB reviews-trained model: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Estimated accuracy of Sentiment140 tweets-trained model: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfor2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'for1' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Estimated accuracy of IMDB reviews-trained model: \", for1/30)\n",
    "print(\"Estimated accuracy of Sentiment140 tweets-trained model: \", for2/30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"out-preds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>imdb_model_pred</th>\n",
       "      <th>s140_model_pred</th>\n",
       "      <th>my_own_eval</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @Mark_Meed: Breaking: Obama Official Linked...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Democrat stimulus plan is a mechanism whos...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>is still trying to figure out why Obama got th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @Lyn_Sue: RT @TellTheTruth1 http://bit.ly/v...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@lauraflyme We need to destroy every company t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet   imdb_model_pred  \\\n",
       "0  RT @Mark_Meed: Breaking: Obama Official Linked...                 1   \n",
       "1  The Democrat stimulus plan is a mechanism whos...                 1   \n",
       "2  is still trying to figure out why Obama got th...                 0   \n",
       "3  RT @Lyn_Sue: RT @TellTheTruth1 http://bit.ly/v...                 0   \n",
       "4  @lauraflyme We need to destroy every company t...                 0   \n",
       "\n",
       "   s140_model_pred  my_own_eval  \n",
       "0                0            0  \n",
       "1                0            0  \n",
       "2                0            0  \n",
       "3                0            0  \n",
       "4                0            0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWEET:  RT @Mark_Meed: Breaking: Obama Official Linked to Racially Charged Boycott of Glenn Beck - HUMAN EVENTS http://bit.ly/53pKk -Infuriating!\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  The Democrat stimulus plan is a mechanism whose goal is the destruction of the traditional American way of life. Nancy Cappock\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  is still trying to figure out why Obama got the nobel peace prize\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  RT @Lyn_Sue: RT @TellTheTruth1 http://bit.ly/v2s9V Sotomayor is nvolved n bkruptcy fraud & she lied. Vote delayd SHE Cn B STOPPED!! #tea ...\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  @lauraflyme We need to destroy every company that took any Obama money. Sink the entire banking sector!\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  YOU crazies are what give women and \"feminism\" a bad rep.\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  RT @worldprayr: U.S. soldier captured in Afghanistan, the military said. Missing since June 30. Pray for his safety and peace for his f ...\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  RT @billmaher: I'd give a weeks pay 4 10min w/Sarah Palin - will someone pls ask her a difficult ? Its fun to watch know-nothings squirm!\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  @Dumb_Ox new congress report out, cf my post tomorrow http://thomistic.blogspot.com // I will. Thank you. #tcot\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  Strange activity,Twitter says account suspended because of strange activity.Hope being Liberal Democrat is not considered strange activity.\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  RT Ed Chen yet another Obama left wing radical/America hating appointee http://is.gd/4GnkV #tcot #tlot #sgp #orca (via @exposeliberals) rope\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  RT @stoptheliberalsplease join the fight. we need everyone's help in fighting this travesty in the Senate! http://bit.ly/o0kxd #stln #tcot\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  RT @wyclef: RT @Fun_All_Day: President Clinton, is meeting in Haiti's capitol 2 promote economic growth and jobs. Things are moving fo ...\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  @senjohnmccain Are you for or against a government-run plan? What will Canadians do, if we wreck our system? #handsoff http://bit.ly/16zQ2P\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  First reading: Obama's Pacific ambitions play well for New Zealand: It warms up the talks that were put on .. http://bit.ly/1ohnWY\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  Thank you!! #tcot #tlot #cleansweep #patriot RT @esser1999: notallysonschwartz.com ...conservatives opposing Rep Allyson Schwartz D\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  @Ubumbo Lou dobbs says avatar is a liberal plot to turn the US more receptive to aliens....smh\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  RT @OTOOLEFAN Government #hcr is so BAD that we only reserve it for our PARENTS, OUR VETERANS, OUR SOLDIERS NOW & OUR LAWMAKERS\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  RT @GStephanopoulos: Graham: Obama 'Timid and Passive' on Iran http://tinyurl.com/lp9rlj Graham is timid/passive one. Obama handlng brilnt\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  The Public Isn't Buying Obama's Smooth Talk.. http://digg.com/d3zHSy\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  @Kasons4 I'm sure Obama takes at least 5 backup teleprompters everywhere he goes. Otherwise he'd be lose w/out them. #tcot #tlot #sgp\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  RT @JeffAtwater \"I, however, place economy among the most important virtues, and public debt as the greatests of the dangers to be feare...\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  1\n",
      "MY OWN EVALUATION:  1\n",
      "-------------------------\n",
      "TWEET:  Who can tax the sunshine sprinkle it with goo mix it up with lies and it make it all taste good? The government can! http://bit.ly/fwD64\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  Who the hell shined the spotlight on Sarah Palin again?? Between her and this twilight foolishness, I'm going to be cranky all week!\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  says B.S. to that. Conservatives only want to make untold profits from human misery and suffering. HEALTH CARE FOR ALL OF US! GO!\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  Pass it on Peeps! RT @GovMikeHuckabee Congress works 4 us! We should see the #healthcare cost B4 the vote! http://tinyurl.com/ljby65 PLS RT!\n",
      "IMDB PREDICTION:  0\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  @LOA_Lover to do everything I possibly can 2 take back the American government & give it back 2 tax paying Americans\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  @VetsOnTheWatch Libs must hate it when conservatives take their ad-hominems and use them as proud monikers to show who we are.\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  #inaug09 Soros: Obama Improves Openness: A fine bit of stenography from the always compliant Associated .. http://bit.ly/pWSOo\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n",
      "TWEET:  EPA Jackson: people want to keep out cold and save on energy bills. \"Environmentalism seems like enclave of the well-off.\" #AGIF\n",
      "IMDB PREDICTION:  1\n",
      "S140 PREDICTION:  0\n",
      "MY OWN EVALUATION:  0\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(df.shape[0]):\n",
    "    print(\"TWEET: \", df.iloc[i, 0])\n",
    "    print(\"IMDB PREDICTION: \", df.iloc[i, 1])\n",
    "    print(\"S140 PREDICTION: \", df.iloc[i, 2])\n",
    "    print(\"MY OWN EVALUATION: \", df.iloc[i, 2])\n",
    "    print(\"-------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
